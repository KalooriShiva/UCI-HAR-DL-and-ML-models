{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sleep stage classification from Respiration signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection Task:\n",
    "\n",
    "Use the UCI HAR dataset and build a model to predict various activity classes.\n",
    "Use Deep Learning models like LSTM, and 1D Cnns for modeling.\n",
    "How can the same be done using Machine Learning models like Random forest, SVM, and Logistic regression?\n",
    "Make a .ipynb file to demonstrate modeling using Deep Learning.\n",
    "Make a .ipynb file to demonstrate modeling using Machine learning.\n",
    "YOU WILL BE ASKED TO EXPLAIN YOUR CODE.\n",
    "Reach out to mentors when in need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D,LSTM, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, BatchNormalization, MaxPool1D, Reshape, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Normalization and one-hot encoding for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is your data and y are your labels\n",
    "def Normalization(X,y):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    y = y -1\n",
    "    # Normalize the data (e.g., Min-Max scaling or Standardization)\n",
    "    X = (X - np.min(X)) / (np.max(X) - np.min(X))  # Min-Max scaling\n",
    "\n",
    "    # If you need to convert your labels to one-hot encoding\n",
    "    y = to_categorical(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are using the time series data for the Deep Learning models as a input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are using the X_train from MakeDataset which was created in the Assignment-1 in Machine Learning course <br>\n",
    "* Both 1D-CNN and LSTM expects the input to be #Dshape means: <br>\n",
    "* [Batch size, time-steps, Features]<br>\n",
    "* so X_train.shape = [126,500,3], in which 126 is the no of smaples and the 500 is the time steps we have (50hz for 10s data), 3 Features accx,accy,accz respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 500, 3) (126, 6) (54, 500, 3) (54, 6)\n"
     ]
    }
   ],
   "source": [
    "from HAR.MakeDataset import X_train, y_train, X_test, y_test\n",
    "ACTIVITIES = {\n",
    "    1: 'WALKING'            ,\n",
    "    2: 'WALKING_UPSTAIRS'   ,\n",
    "    3: 'WALKING_DOWNSTAIRS' ,\n",
    "    4: 'SITTING'            ,\n",
    "    5: 'STANDING'           ,\n",
    "    6: 'LAYING'             ,\n",
    "}\n",
    "# X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
    "# X_test = X_test.reshape((X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
    "y_train_n = y_train\n",
    "y_test_n = y_test\n",
    "X_train ,y_train = Normalization(X_train,y_train)\n",
    "X_test, y_test = Normalization(X_test,y_test)\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaion of 1D-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1D-CNN implementaion with 2 layers of 1D-CNN <br>\n",
    "* In between this 2 layers 2 times of MaxPooling for the reduction of the size.\n",
    "* Apprently we have 100 dense layers\n",
    "* At last we have ouput layer with 6 ouputs for 6-activites and applied softmax activiation\n",
    "* Then we are using adam optimizer and loss as categorical loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th run\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "1th run\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2th run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 13ms/step\n",
      "3th run\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "4th run\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "5th run\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "6th run\n",
      "2/2 [==============================] - 0s 23ms/step\n",
      "7th run\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "8th run\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "9th run\n",
      "2/2 [==============================] - 0s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_runs = 10\n",
    "acc = []\n",
    "pre = []\n",
    "recall = []\n",
    "f1_scores = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    print(f'{i}th run')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64,kernel_size=2,activation='relu',input_shape=(500,3)))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Conv1D(filters=64,kernel_size=2,activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(len(ACTIVITIES), activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(X_train,y_train,epochs=20,batch_size=32,validation_data=(X_test,y_test),verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred,axis=1)\n",
    "    y_true = np.argmax(y_test,axis=1)\n",
    "    acc.append(accuracy_score(y_true,y_pred_classes))\n",
    "    pre.append(precision_score(y_true,y_pred_classes, average='weighted'))\n",
    "    recall.append(recall_score(y_true,y_pred_classes, average='weighted'))\n",
    "    f1_scores.append(f1_score(y_true,y_pred_classes, average='weighted'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of all evalution metrics:\n",
      "Accuracy :0.6055555555555556\n",
      "Precision :0.5963051395771984\n",
      "Recall :0.6055555555555556\n",
      "F1 Score :0.5870995014819267\n"
     ]
    }
   ],
   "source": [
    "print(f'Average of all evalution metrics:')\n",
    "print(f'Accuracy :{np.mean(acc)}')\n",
    "print(f'Precision :{np.mean(pre)}')\n",
    "print(f'Recall :{np.mean(recall)}')\n",
    "print(f'F1 Score :{np.mean(f1_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implemetaion of the LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this also we have 2 layers of LSTM and then Dense and output as as CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th run\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 6s 645ms/step - loss: 1.7899 - accuracy: 0.1667 - val_loss: 1.7870 - val_accuracy: 0.1667\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 2s 509ms/step - loss: 1.7834 - accuracy: 0.1667 - val_loss: 1.7813 - val_accuracy: 0.1667\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 2s 495ms/step - loss: 1.7768 - accuracy: 0.2222 - val_loss: 1.7746 - val_accuracy: 0.2593\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 2s 495ms/step - loss: 1.7693 - accuracy: 0.2857 - val_loss: 1.7667 - val_accuracy: 0.2963\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 2s 499ms/step - loss: 1.7605 - accuracy: 0.3016 - val_loss: 1.7556 - val_accuracy: 0.3333\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 2s 495ms/step - loss: 1.7454 - accuracy: 0.3095 - val_loss: 1.7389 - val_accuracy: 0.3333\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 2s 491ms/step - loss: 1.7262 - accuracy: 0.3254 - val_loss: 1.7144 - val_accuracy: 0.3333\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 2s 484ms/step - loss: 1.6950 - accuracy: 0.3254 - val_loss: 1.6709 - val_accuracy: 0.3333\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 2s 492ms/step - loss: 1052183449166859140031527780352.0000 - accuracy: 0.3016 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 2s 489ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 2s 505ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 2s 506ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 2s 512ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 2s 509ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 2s 508ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 2s 503ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 2s 500ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 2s 508ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 2s 501ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 2s 494ms/step - loss: nan - accuracy: 0.1667 - val_loss: nan - val_accuracy: 0.1667\n",
      "2/2 [==============================] - 1s 114ms/step\n"
     ]
    }
   ],
   "source": [
    "n_runs = 1\n",
    "acc_LSTM = []\n",
    "pre_LSTM = []\n",
    "recall_LSTM = []\n",
    "f1_LSTM = []\n",
    "for i in range(n_runs):\n",
    "    print(f'{i}th run')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32,return_sequences=True,input_shape=(500,3),activation='relu'))\n",
    "    model.add(LSTM(32,activation= 'relu'))\n",
    "    \n",
    "    model.add(Dense(len(ACTIVITIES),activation= 'softmax'))\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred,axis=1)\n",
    "    y_true = np.argmax(y_test,axis=1)\n",
    "    acc_LSTM.append(accuracy_score(y_true,y_pred_classes))\n",
    "    pre_LSTM.append(precision_score(y_true,y_pred_classes, average='weighted'))\n",
    "    recall_LSTM.append(recall_score(y_true,y_pred_classes, average='weighted'))\n",
    "    f1_LSTM.append(f1_score(y_true,y_pred_classes, average='weighted'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of all evalution metrics:\n",
      "Accuracy :0.16666666666666666\n",
      "Precision :0.027777777777777776\n",
      "Recall :0.16666666666666666\n",
      "F1 Score :0.047619047619047616\n"
     ]
    }
   ],
   "source": [
    "print(f'Average of all evalution metrics:')\n",
    "print(f'Accuracy :{np.mean(acc_LSTM)}')\n",
    "print(f'Precision :{np.mean(pre_LSTM)}')\n",
    "print(f'Recall :{np.mean(recall_LSTM)}')\n",
    "print(f'F1 Score :{np.mean(f1_LSTM)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.isinf(X_train).any())\n",
    "print(np.isinf(y_train).any())\n",
    "print(np.isinf(X_test).any())\n",
    "print(np.isinf(y_test).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "* So 1D-CNN is working fine but LSTM not it is reaching loss = infinity after few iteration of the epochs.<br>\n",
    "* So I changed the Learning rate to 0.01 to 0.0001 but still its going to the inifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Material implemenation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is the code where the combination of 1D-CNN and LSTM are used so I have used the code from the reading material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_177\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_54 (LSTM)              (None, 1500, 32)          4352      \n",
      "                                                                 \n",
      " lstm_55 (LSTM)              (None, 1500, 32)          8320      \n",
      "                                                                 \n",
      " reshape_17 (Reshape)        (None, 1, 1500, 32)       0         \n",
      "                                                                 \n",
      " conv1d_307 (Conv1D)         (None, 1, 750, 64)        4160      \n",
      "                                                                 \n",
      " reshape_18 (Reshape)        (None, 750, 64)           0         \n",
      "                                                                 \n",
      " max_pooling1d_302 (MaxPooli  (None, 188, 64)          0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_308 (Conv1D)         (None, 187, 192)          24768     \n",
      "                                                                 \n",
      " reshape_19 (Reshape)        (None, 187, 192)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d_7   (None, 192)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 192)              768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_312 (Dense)           (None, 6)                 1158      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,526\n",
      "Trainable params: 43,142\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_CL = Sequential()\n",
    "model_CL.add(LSTM(32, return_sequences=True, input_shape=(500,3), activation='relu'))\n",
    "model_CL.add(LSTM(32,return_sequences=True, activation='relu'))\n",
    "model_CL.add(Reshape((1, 500, 32)))\n",
    "model_CL.add(Conv1D(filters=64,kernel_size=2, activation='relu', strides=2))\n",
    "model_CL.add(Reshape((250, 64)))\n",
    "model_CL.add(MaxPool1D(pool_size=4, padding='same'))\n",
    "model_CL.add(Conv1D(filters=192, kernel_size=2, activation='relu', strides=1))\n",
    "model_CL.add(Reshape((62, 192)))\n",
    "model_CL.add(GlobalAveragePooling1D())\n",
    "model_CL.add(BatchNormalization(epsilon=1e-06))\n",
    "model_CL.add(Dense(6))\n",
    "model_CL.add(Activation('softmax'))\n",
    "\n",
    "print(model_CL.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 9s 9s/step - loss: 1.8540 - accuracy: 0.1190 - val_loss: 1.7914 - val_accuracy: 0.1667\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.6919 - accuracy: 0.3254 - val_loss: 1.7913 - val_accuracy: 0.1852\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.6138 - accuracy: 0.3254 - val_loss: 1.7913 - val_accuracy: 0.1667\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5753 - accuracy: 0.3254 - val_loss: 1.7912 - val_accuracy: 0.1667\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5456 - accuracy: 0.3254 - val_loss: 1.7912 - val_accuracy: 0.1667\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5284 - accuracy: 0.3254 - val_loss: 1.7912 - val_accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5116 - accuracy: 0.3254 - val_loss: 1.7912 - val_accuracy: 0.1667\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5026 - accuracy: 0.3095 - val_loss: 1.7913 - val_accuracy: 0.1667\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4940 - accuracy: 0.3413 - val_loss: 1.7913 - val_accuracy: 0.2222\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4916 - accuracy: 0.3095 - val_loss: 1.7914 - val_accuracy: 0.1667\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4829 - accuracy: 0.3333 - val_loss: 1.7916 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4725 - accuracy: 0.3651 - val_loss: 1.7918 - val_accuracy: 0.1667\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4687 - accuracy: 0.3095 - val_loss: 1.7920 - val_accuracy: 0.1667\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4592 - accuracy: 0.3095 - val_loss: 1.7924 - val_accuracy: 0.1667\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4331 - accuracy: 0.3968 - val_loss: 1.7929 - val_accuracy: 0.1667\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4264 - accuracy: 0.3810 - val_loss: 1.7934 - val_accuracy: 0.1667\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4280 - accuracy: 0.4127 - val_loss: 1.7940 - val_accuracy: 0.1667\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4187 - accuracy: 0.4286 - val_loss: 1.7947 - val_accuracy: 0.1667\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4066 - accuracy: 0.4206 - val_loss: 1.7955 - val_accuracy: 0.1667\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-a44181da828a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m192\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                    )\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m--> 135\u001b[1;33m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\Shiva\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 53\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_CL.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_CL.fit(X_train,\n",
    "                    y_train, \n",
    "                    batch_size= 192, \n",
    "                    epochs=100,\n",
    "                    validation_data=(X_test, y_test)\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 142ms/step\n",
      "Results of all evalution metrics:\n",
      "Accuracy :0.16666666666666666\n",
      "Precision :0.027777777777777776\n",
      "Recall :0.16666666666666666\n",
      "F1 Score :0.047619047619047616\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_CL.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred,axis=1)\n",
    "y_true = np.argmax(y_test,axis=1)\n",
    "\n",
    "acc_CL = (accuracy_score(y_true,y_pred_classes))\n",
    "pre_CL =(precision_score(y_true,y_pred_classes, average='weighted'))\n",
    "recall_CL = (recall_score(y_true,y_pred_classes, average='weighted'))\n",
    "f1_score_CL = (f1_score(y_true,y_pred_classes, average='weighted'))\n",
    "\n",
    "print(f'Results of all evalution metrics:')\n",
    "print(f'Accuracy :{acc_CL}')\n",
    "print(f'Precision :{pre_CL}')\n",
    "print(f'Recall :{recall_CL}')\n",
    "print(f'F1 Score :{f1_score_CL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still the results where not better I will look in to it more deeper I am currently learning LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
